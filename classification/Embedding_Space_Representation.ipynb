{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "Use similarity score of input text and category description to predict input text's category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial phase:\n",
    "1. Create text description for each category.\n",
    "2. Get the embeddings of each cateogry description.\n",
    "Inference phase:\n",
    "1. Get embedding of the input text.\n",
    "2. Calculate similarity of input text's embedding against category description embeddings.\n",
    "3. Based upon similarity score, predict category of the input text.\n",
    "\n",
    "Fine tuning:\n",
    "1. Improvise category description.\n",
    "2. User better model for getting embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = {\n",
    "#     \"affirm\": \"Text that expresses agreement, confirmation, or a positive response. Examples: 'Yes, I agree,' 'That’s correct,' or 'Sure, I’ll do it.'\",\n",
    "#     \"deny\": \"Text that expresses disagreement, refusal, or a negative response. Examples: 'No, that’s not right,' 'I disagree,' or 'I can’t do it.'\",\n",
    "#     \"not_sure\": \"Text that expresses uncertainty, doubt, or hesitation. Examples: 'I’m not sure,' 'Maybe,' or 'I need more information.'\"\n",
    "# }\n",
    "categories = {\n",
    "    \"affirm\": \"agreement confirmation positive Yes, I agree That’s correct or Sure I’ll do it.\",\n",
    "    \"deny\": \"disagreement refusal negative No, that’s not right I disagree I can’t do it.\",\n",
    "    \"not_sure\": \"uncertainty doubt hesitation I’m not sure Maybe I need more information.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = [\n",
    "    # Affirmative Examples\n",
    "    {\"text\": \"Yes, that makes perfect sense.\", \"category\": \"affirm\"},\n",
    "    {\"text\": \"I completely agree with you.\", \"category\": \"affirm\"},\n",
    "    {\"text\": \"Absolutely, that’s correct.\", \"category\": \"affirm\"},\n",
    "    {\"text\": \"Sure thing, I’ll handle it.\", \"category\": \"affirm\"},\n",
    "\n",
    "    # Negative Examples\n",
    "    {\"text\": \"No, I don’t think so.\", \"category\": \"deny\"},\n",
    "    {\"text\": \"That’s not what I meant at all.\", \"category\": \"deny\"},\n",
    "    {\"text\": \"I can’t agree with this.\", \"category\": \"deny\"},\n",
    "    {\"text\": \"No way, that’s incorrect.\", \"category\": \"deny\"},\n",
    "\n",
    "    # Uncertainty Examples\n",
    "    {\"text\": \"I’m not sure if this is right.\", \"category\": \"not_sure\"},\n",
    "    {\"text\": \"Maybe, but I need more information.\", \"category\": \"not_sure\"},\n",
    "    {\"text\": \"It could be true, but I’m not confident.\", \"category\": \"not_sure\"},\n",
    "    {\"text\": \"I don’t know for certain.\", \"category\": \"not_sure\"},\n",
    "\n",
    "    # Ambiguous/Edge Cases\n",
    "    {\"text\": \"Possibly, but I’ll have to think about it.\", \"category\": \"not_sure\"},  # Uncertain tone\n",
    "    {\"text\": \"No, wait... actually, yes.\", \"category\": \"affirm\"},  # Affirmative conclusion\n",
    "    {\"text\": \"I don’t think I agree with that, but I’m not certain.\", \"category\": \"not_sure\"},  # Mixed tone\n",
    "    {\"text\": \"I’m sorry, I can’t do that.\", \"category\": \"deny\"},  # Negative with apology\n",
    "    {\"text\": \"Yes, but I’m still a bit unsure.\", \"category\": \"affirm\"},  # Affirmative with slight doubt\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Phase\n",
    "Getting embeddings of category descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ollama\n",
    "ollama exposes `/api/embeddings` api endpoint which we can use to get embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama  # pip install ollama\n",
    "\n",
    "# MODEL = \"nomic-embed-text\"\n",
    "MODEL = \"mxbai-embed-large\"\n",
    "HOST = \"http:/localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_embeddings = {\n",
    "    category: ollama.embed(model=MODEL, input=description)[\"embeddings\"][0]\n",
    "    for category, description in categories.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "Classify the category of input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'affirm'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def cal_similarity(v1, v2):\n",
    "    return cosine_similarity([v1], [v2])\n",
    "\n",
    "\n",
    "def classify(input_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input (str): The input text that is to be classified.\n",
    "    Returns:\n",
    "        str: The predicted class of input text.\n",
    "    \"\"\"\n",
    "    input_text_embedding = ollama.embed(model=MODEL, input=input_text)[\"embeddings\"][0]\n",
    "    similarities = {\n",
    "        category: cal_similarity(input_text_embedding, category_embedding)\n",
    "        for category, category_embedding in category_embeddings.items()\n",
    "    }\n",
    "    return max(similarities, key=similarities.get)\n",
    "\n",
    "\n",
    "classify(\"Yea sure!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affirm : affirm : Yes, that makes perfect sense.\n",
      "affirm : affirm : I completely agree with you.\n",
      "affirm : affirm : Absolutely, that’s correct.\n",
      "affirm : affirm : Sure thing, I’ll handle it.\n",
      "deny : deny : No, I don’t think so.\n",
      "deny : deny : That’s not what I meant at all.\n",
      "deny : deny : I can’t agree with this.\n",
      "deny : deny : No way, that’s incorrect.\n",
      "not_sure : not_sure : I’m not sure if this is right.\n",
      "not_sure : not_sure : Maybe, but I need more information.\n",
      "not_sure : not_sure : It could be true, but I’m not confident.\n",
      "not_sure : not_sure : I don’t know for certain.\n",
      "not_sure : not_sure : Possibly, but I’ll have to think about it.\n",
      "affirm : affirm : No, wait... actually, yes.\n",
      "not_sure : not_sure : I don’t think I agree with that, but I’m not certain.\n",
      "deny : deny : I’m sorry, I can’t do that.\n",
      "not_sure : affirm : Yes, but I’m still a bit unsure.\n"
     ]
    }
   ],
   "source": [
    "for sample in test_samples:\n",
    "    predicted = classify(sample[\"text\"])\n",
    "    print(f'{predicted} : {sample[\"category\"]} : {sample[\"text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
